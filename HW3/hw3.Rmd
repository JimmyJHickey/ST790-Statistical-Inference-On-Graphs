---
title: "ST 790 Assignment 3"
author: "David Elsheimer and Jimmy Hickey"
date: "10/15/2020"
output: pdf_document
urlcolor: blue

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instruction
This assignment consists of $3$ problems. The assignment is due on 
**Friday, October 16** at 11:59pm EDT. Please submit your assignment electronically through the **Moodle** webpage.
The assignment can be done as a group with at most 3 members per group (please include the name of the group members on the front page of the assignment).

# Problem 1
In class, we only proved matrix concentration inequalities for symmetric matrices. Specifically,
Theorem 9 in the lecture notes states that

**Theorem** (Tropp) Let $X_1, X_2, \dots, X_n$ be independent mean $0$ $d \times d$ **symmetric** random matrices and suppose that $\|X_i\| \leq M$ almost surely for all $i$. Define 
$$\sigma^2 = \Bigl\|\sum_{i=1}^{n} \mathbb{E}[X_i^2] \Bigr\|$$
Then for every $t > 0$
$$\mathbb{P}\Bigl(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t\Bigr) \leq 2d \exp\Bigl(\frac{-t^2}{\sigma^2 + Mt/3}\Bigr)$$
Extend this result to general matrices with real-valued entries. That is, show the following corollary

**Corollary** Let $X_1, X_2, \dots, X_n$ be independent mean $0$ $d_1 \times d_2$ random matrices and suppose that $\|X_i\| \leq M$ almost surely for all $i$. Define
$$\sigma^2 = \max\Bigl\{\Bigl\|\sum_{i=1}^{n} \mathbb{E}[X_i X_i^{\top}]\Bigr\|, \Bigl\|\sum_{i=1}^{n} \mathbb{E}[X_i^{\top} X_i]\Bigr\|\Bigr\}.$$
Then for every $t > 0$
$$\mathbb{P}\Bigl(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t\Bigr) \leq (d_1 + d_2) \exp\Bigl(\frac{-t^2}{\sigma^2 + Mt/3}\Bigr).$$
**Hint** Relate the singular values of a rectangular matrix $M$ with the eigenvalues of its **symmetric dilation**
$$\begin{bmatrix} 0 & M \\ M^{\top} & 0 \end{bmatrix}.$$
**Answer:**

Note that the Hermitean dilation $H = \begin{bmatrix} 0 & M \\ M^{\top} & 0 \end{bmatrix}$ is symmetric. Then note that $H^2 = H\cdot H = \begin{bmatrix} MM^{\top} & 0 \\ 0&  M^{\top} M \end{bmatrix}$. This is a symmetric, block diagonal matrix. The eigenvalues of $H^2$ are the eigenvalues of $M M^{\top}, M^{\top}M$. In turn, these are the same as the quare of the singular values of $M, M^{\top}$. As such, the spectral radius of $M$ is the same as the largest eigenvalues of $H$.

Where H is as previously defined, we can then consider $H = \displaystyle \sum_{i=1}^n H_i$, such that $H_i = \begin{bmatrix}0 & X_i \\ X_i^{\top} & 0\end{bmatrix}$.

Using this, the proof follows similarly to the proof for Tropp.

Note first that $\Bigl\|\displaystyle\sum_{i=1}^{n} X_i\Bigr\| = \Bigl\|\sum_{i=1}^{n} H_i\Bigr\| = \displaystyle\lambda_{max}(Z_n)$, where $Z_n = \displaystyle\sum_{i=1}^{n} H_i$ . 

Note further that $\sigma^2 = \max\Bigl\{\Bigl\|\displaystyle\sum_{i=1}^{n} \mathbb{E}[X_i X_i^{\top}]\Bigr\|, \Bigl\|\displaystyle\sum_{i=1}^{n} \mathbb{E}[X_i^{\top} X_i]\Bigr\|\Bigr\} = \Bigl \|\displaystyle\sum_{i=1}^nH_i^2\Bigr\|$.

Thus, the remainder of the proof is as follows 


\begin{align*}
P(\Bigl\|\sum_{i=1}^{n} H_i\Bigr\| > t) & \leq e^{-st}E(\exp (s\lambda_{max}(Z_n)))\\
&\leq e^{-st}E(\text{tr }(\exp(\sum_{i=1}^n s H_i)))\\
& = e^{-st}E(E(\text{tr } \exp\left(\sum_{i=1}^{n-1}s H_i + sH_n\right)|\{H_i\}_{i=1}^{n-1}))\\
& \leq e^{-st}E(\text{tr } \exp\left(\sum_{i=1}^{n-1}s H_i\right) + \log E(e^{s H_n}))\\
& \leq \cdots\\
& \leq e^{-st}\text{tr } \exp\left(\sum_{i=1}^{n} \log E(e^{s H_i}) \right), \ [\text{Via corollary 13.}]\\
& \leq e^{-st}\text{tr }\exp\left(\sum_{i=1}^n \frac{s^2/2}{1-|s|M/3} E(H_i^2)\right), \ [\text{Via Proposition 14.}]\\
& \leq (d_1+d_2)e^{-st}\lambda_{max}\left(\exp(\frac{s^2/2}{1-|s|M/3}\sum_{i=1}^n  E(H_i^2))\right), \ [\text{Via Golden-Thompson.}]\\
& \leq (d_1+d_2)e^{-st}\exp\left(\frac{s^2/2}{1-|s|M/3}\sum_{i=1}^n  \lambda_{max}(E(H_i^2))\right)\\
& = (d_1+d_2)e^{-st}\exp\left(\frac{s^2/2}{1-|s|M/3} \Bigl\|\sum_{i=1}^n H_i^2\Bigr\|\right)\\
& = (d_1+d_2)e^{-st}\exp\left(\frac{\sigma^2s^2/2}{1-|s|M/3} \right)\\
\end{align*}

For $s = \frac{t}{\sigma^2 + Mt/3}$, this bound is $(d_1+d_2)\exp(\frac{-t^2}{\sigma^2 + Mt/3})\exp\left(t^2/2 \right)$ which is always greater than or equal to $(d_1+d_2)\exp(\frac{-t^2}{\sigma^2 + Mt/3})$.

Thus for independent matrices $X_i$ with dimension $d_1\times d_2$, $P(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t) \leq (d_1+d_2)\exp(\frac{-t^2}{\sigma^2 + Mt/3})$.




\newpage

# Problem 2
To continue with the theme of the previous problem, in our class we only prove matrix concentration inequalities for the case when the summands are bounded almost surely in spectral norm. See
for example the statement of the result in the previous problem. 
For this problem, you are asked to derive a matrix concentration
inequality in the setting where the matrix powers of $X_i$ satisfy a Bernstein-like tail conditions. In particular,
prove the following result.

**Theorem** Let $X_1, X_2, \dots, X_n$ be independent mean $0$, symmetric random $d \times d$ matrices. Furthermore, assume that there exists a constant $R > 0$ and symmetric matrices $A_1, A_2, \dots, A_n$ such that for any $i$
$$\mathbb{E}[X_i^{p}] \preceq \frac{p!}{2} R^{p-2} A_i^2, \quad \text{for $p = 2,3,4,\dots$}.$$
Here $A \preceq B$ if $B - A$ is a positive semidefinite matrix. Now define
$$\sigma^2 = \Bigl\|\sum_{i=1}^{n} A_i^2 \Bigr\|$$
Then for every $t > 0$
$$\mathbb{P}\Bigl(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t\Bigr) \leq 2d \exp\Bigl(\frac{-t^2/2}{\sigma^2 + Rt}\Bigr)$$  

Next apply the above result to derive a tail bound for the following (weighted) random graph model. Let $P$ be
a $n \times n$ symmetric matrix with non-negative, finite entries $p_{ij}$ . Let A be a $n \times n$ symmetric matrix whose entries $a_{ij}$ for $i \leq j$ are independent and that $a_{ij} \sim \mathrm{Pois}(p_{ij})$. 
Sketch a tail bound for $\|A - P\|$ similar in flavor to Theorem $12$ in the set of lecture notes for matrix concentration inequalities. (You donâ€™t have to pay too much attention to constant
factors; we are only interested in important quantities similar to the $\sqrt{\Delta \log n}$ in the statement of Theorem $12$ there).

**Answer:**

The first portion of this problem requires a proof follows similarly to the proof for theorem 11, with some minor adjustments. However, to avoid any confusion from skipping steps, the full proof of this portion is provided. Note $Z_n = \displaystyle \sum_{i=1}^n X_i$, $E(X_i^2) = A_i^2$.

\begin{align*}
P(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t) & \leq e^{-st}E(\exp (s\lambda_{max}(Z_n)))\\
&\leq e^{-st}E(\text{tr }(\exp(\sum_{i=1}^n s X_i)))\\
& = e^{-st}E(E(\text{tr } \exp\left(\sum_{i=1}^{n-1}s X_i + sX_n\right)|\{X_i\}_{i=1}^{n-1}))\\
& \leq e^{-st}E(\text{tr } \exp\left(\sum_{i=1}^{n-1}s X_i\right) + \log E(e^{s X_n}))\\
& \leq \cdots\\
& \leq e^{-st}\text{tr } \exp\left(\sum_{i=1}^{n} \log E(e^{s X_i}) \right), \ [\text{Via corollary 13.}]\\
& \leq e^{-st}\text{tr }\exp\left(\sum_{i=1}^n \frac{s^2/2}{1-|s|M/3} E(X_i^2)\right), \ [\text{Via Proposition 14.}]\\
& \leq de^{-st}\lambda_{max}\left(\exp(\frac{s^2/2}{1-|s|M/3}\sum_{i=1}^n  E(X_i^2))\right), \ [\text{Via Golden-Thompson.}]\\
& \leq de^{-st}\exp\left(\frac{s^2/2}{1-|s|M/3}\sum_{i=1}^n  \lambda_{max}(E(X_i^2))\right)\\
& = de^{-st}\exp\left(\frac{s^2/2}{1-|s|M/3} \Bigl\|\sum_{i=1}^n A_i^2\Bigr\|\right)\\
& = de^{-st}\exp\left(\frac{\sigma^2s^2/2}{1-|s|M/3} \right)\\
\end{align*}

For $s = \frac{t/2}{\sigma^2+R t}$, where $R = M/3$, the theorem holds for the above bound.

Thus for independent matrices $X_i$ with dimension $d\times d$, $P(\Bigl\|\sum_{i=1}^{n} X_i\Bigr\| > t) \leq 2d\exp(\frac{-t^2/2}{\sigma^2 + Rt})$.

Using this result, the second part of the problem involves finding a bound for the weighted graph model as defined in the problem. Note the following:

$$
\zeta_{ij} = \begin{cases} 
      e_ie_j^\top + e_je_i^\top & i\neq j\\
      e_ie_i^\top & i = j\\
   \end{cases}
$$
For $\zeta_{ij}, e_1, \dots, e_n$ are the basic vectors of $\mathbb{R}^n$.

Note $A-P = \displaystyle\sum_{i\leq j}(a_{ij}-p_{ij})\zeta_{ij}$. Let $X_{ij}=(a_{ij}-p_{ij})\zeta_{ij}$.

Then $P(\Bigl\|A-P\Bigr\|  \geq t) \leq 2n \exp\left(\frac{-t^2/2}{\sigma^2 + Rt}\right)$.

\begin{align*}
\sigma^2 & =  \Bigl\|E(X_{ij}^2)\Bigr\|\\
& = p_{ij}\zeta_{ij}^2\\
\sum_{i\leq j}E(X_{ij}^2) & = \sum_{i=1}^n\sum_{j=1}^np_{ij}e_ie_i^\top\\
\sigma^2 &=\underset{i}{\max}\sum_{j}p_{ij} = \Delta\\
P(\Bigl\|A-P\Bigr\|  \geq t) &\leq 2n \exp\left(\frac{-t^2/2}{\sigma^2 + Rt}\right)\\
& = 2n \exp\left(\frac{-t^2}{2\sigma^2 + 2Rt}\right)
\end{align*}

Let $c>0$, assume $\eta \in [n^{-c},1/2]$. Then there exists $C$ independent from $n, P$ such that for $\Delta > C\log n, RT = 2\sqrt{\Delta \log(2n/\eta)} < \Delta$. Then,


\begin{align*}
P(\Bigl\|A-P\Bigr\|  \geq t) &\leq 2n \exp\left(\frac{-t^2}{2\Delta + 2\Delta}\right)\\
&\leq 2n \exp\left(\frac{-t^2}{4\Delta}\right)\\
&= 2n\exp(-\log(2n\eta))\\
&=\eta
\end{align*}

Thus $\eta$ is a bound for $P(\Bigl\|A-P\Bigr\|  \geq t).$


\newpage

# Problem 3
Sketch a proof for the weak recovery of SBMs for the spectral clustering using the normalized Laplacian. In particular, consider the algorithm on page $1881$ of [Rohe et al.](https://projecteuclid.org/download/pdfview_1/euclid.aos/1314190618). Note that you do not need to use the eigenvectors associated with the largest $k$ eigenvalues (in modulus), but rather the eigenvectors associated with the $d$ largest eigenvalues where $d = \mathrm{rk}(\mathbf{B})$ which is assumed known.

In particular, proceed as follows

+ Relate the eigenvectors of $\mathcal{L}(\mathbf{P}) = \mathbf{T}^{-1/2} \mathbf{P} \mathbf{T}^{-1/2}$ to the block structures. Here $\mathbf{T}$ is a diagonal matrix with $T_{ii} = \sum_{j} p_{ij}$ is the **expected degree** of vertex $i$.

+ Next invoke matrix concentration inequality for $\|\mathcal{L}(\mathbf{A}) - \mathcal{L}(\mathbf{P}) \|$.

+ Then invoke the Davis-Kahan theorem to bound the difference between the top eigenvectors of $\mathcal{L}(\mathbf{A})$ and the top eigenvectors of $\mathcal{L}(\mathbf{P})$.

+ Finally, show that the above difference yield that the **proportion** of mis-clustered vertices (assuming clustering is done via $K$-means)  will converges to $0$.

**Answer:**
We will **very** closely follow the "Implication for our spectral clustering story" from the notes which shows theorem 4 for weak recvover of SBMs.


Take $A \sim SBM(B, \tau)$. Then our normalized Laplacian is $\mathcal L (A) = D^{-1/2} A D^{-1/2}$. Here $D$ is the diagonal matrix with $D_{ii} = \sum_{j} w_{ij}$ where $w_{ij}$ is the edge weight between vertex $i$ and vertex $j$.  Notice that $T$ is the diagonal matrix with th expected degrees as diagonal entries. That is, $E(D_{ii} = T_{ii})$. 

Further take $Z$ to be the $n \times K$ matrix such that $Z_{ik} = I( \tau(i) = k)$. Then we define $P = Z B Z^T$. Then, $\mathcal L (P) = T^{-1/2} P T^{-1/2}$. 


We can order our eigenvalues similar to as we did in the notes. Let $| \widehat \lambda_1 |  \geq |\widehat \lambda _2 | \geq \dots$ be the eigenvalues of $\mathcal L (A)$ in decreasing modulus. Similarly take $|\lambda_1| \geq |\lambda_2| \geq \dots$ to be the eigenvalues of $\mathcal L(P)$ in decreasing modulus. Here $\lambda_s(P) = 0$ for $s > d = \text{rank}( \mathcal L(P) )$. 

Let us definte the sets $S_1 = \{ \widehat \lambda_j : j \geq d+1\}$ and $S_2 = \{\lambda_j : 1 \leq j \leq d \}$. Now we can apply Weyl's inequality to say

$$
\max_{j} | \lambda_j - \widehat \lambda_j | \leq \| \mathcal L(A) - \mathcal L(P) \| \Rightarrow | \widehat \lambda_h |  \leq \| \mathcal L(A) - \mathcal L(P) \| \text{ for all } j \geq d+1.
$$


Let's take $\widehat U$ and $U$ to be the matrices whose columns are the top $d$ eigenvectors of $\mathcal L(A)$ and $\mathcal L(P)$ respectively. Take $\lambda_* = \min \{ |\lambda | : \lambda \in S_2 \}$. Now suppose $\lambda_* > \|\mathcal L(A) - \mathcal L(P) \|$. We can apply the Davis-Kahan Theorem to say

$$
\min_{W \in \mathcal O(p)} \| \widehat U W - U \|_F \leq \frac{ \sqrt{2d} \| \mathcal L(A) - \mathcal L(P) \| }{  \lambda_* - \| \mathcal L(A) - \mathcal L(P) \|}.
$$

We will proceed by bounding $\lambda_*$.

Let's look a general eigenvector $v$ of $\mathcal L(P)$ with eigenvalue $\lambda$. We want to relate these eigenvalues to eigenvalues of a function of the block structure, $B$.

\begin{align*}
\ell (P) v & = \lambda v \\
T^{-1/2} P T^{-1/2} v & = \lambda v \\
T^{-1/2} (Z B Z^T) T^{-1/2} v & = \lambda v \\
BZT^{-1/2} \cdot T^{-1/2} (Z B Z^T) T^{-1/2} v & = BZT^{-1/2} \cdot \lambda v \\
BZT^{-1} Z (B Z^T T^{-1/2} v) & = \lambda (B Z^T T^{-1/2} v)
\end{align*}


Thus, $\mathcal L(P)$ and $B Z^T T^{-1} Z$ share the same eigenvalues. Notice that this is similar to what we found in lecture, except for the $T^{-1}$. Remember that $T$ is diagonal, so this gives us

$$
B Z^T T^{-1} Z = B \times diag(\frac{  n_1}{ T_{11} }, \frac{n_2  }{ T_{22} }, \dots , \frac{  n_k}{ T_{kk} }).
$$

Now assume $n_k = \Theta(n)$ as $n$ increases. This restricts all blocks to grow at the same rate, $n$. Take $d_{min} = \min_i \sum_j p_{ij} = \min_i T_{ii}$. Then we have

$$
\lambda_* = \Theta(n \cdot \lambda_{min}(B) \cdot d_{min}).
$$

Where the $\min_i T_{ii}$ term comes from our additional diagonal terms. Then we can apply the Oliveira's concentraion of Laplacian matrices inequality (theorem 15) to say

$$
P [ \| \mathcal L(A) - \mathcal L(P) \| ] > 14 d_{min}^{-1/2} \sqrt{\log(4n / \eta)  }.
$$

Which implies

$$
P [ \| \mathcal L(A) - \mathcal L(P) \| ] = \sqrt{  \frac{ \log(n)}{  d_{min}}}.
$$


Suppose now that $B = \rho_n \widetilde B$ where $\widetilde B$ is fixed in $n$ with $\rho_n$ possible converging to 0 as $n\rightarrow \infty$. With $\widetilde B$ fixed, B depends on $n$ only through $\rho_n$. So we have the following with high probability. 

$$
\lambda_* = \Theta(n d^{-1}_{min} \rho_n) \text{ and } \| \mathcal L(A) - \mathcal L(P) \| = \Theta( \sqrt{ n d^{-1}_{min} \rho_n })
$$

Further, with high probability we have 

$$
\lambda_* > \| \mathcal L(A) - \mathcal L(P) \|.
$$

We can now bound our Davis-Kahan inequality with high probability.

$$
\min_{W \in \mathcal O(p)} \| \widehat U W - U \|_F \leq \frac{ \sqrt{2d} \| \mathcal L(A) - \mathcal L(P) \| }{  \lambda_* - \| \mathcal L(A) - \mathcal L(P) \|} = O\Big(\sqrt{  d \cdot d_{min}^{-1} } (n \rho_n)^{-1/2} \Big)
$$

Thus, there exists an orthogonal matrix $W_*$ such that, on average

$$
\| W_* \widehat U_i - U_i \| = O\Big(\sqrt{  d \cdot d_{min}^{-1} } (n \rho_n)^{-1/2} \Big).
$$


Note that clustering using $\widehat U$ is "equivalent" to clustering using $\widehat U W$ for any orthogonal $W$. 


Let us now consider $U$, which contains $K$ distinct rows. Since the columns of $U$ are orthonormal, we have for all $i$ with $Z_i \neq Z_j$

$$
\| U_i - Uj \| = \Omega(n^{-1/2}).
$$


Let $S$ be the set of indices such that 

$$
\| W_* \widehat U_i - U_i \| > \min_{k: Z_k \neq Z_i}\| W_* \widehat U_i - U_k \|.
$$


Then $i \in S$ only if $\| W_* \widehat U_i - U_i \| = \Omega(n^{-1/2})$.


We therefore have

\begin{align}
\min W \in \mathcal O(p) \| \widehat U W - U \|^2_F & \geq \sum_{i \in S} \| W_* \widehat U_i - U_i \|^2 \\
  & \geq \sum_{i \in S} \Omega(n^{-1}) \\
  & = |S| \times \Omega(n^{-1}).
\end{align}

Finally, we can use the bound the we derived previously to say

$$
\frac{  |S|}{ n } = O(d d^{-1}_{min} (n \rho)^{-1}) = o(1)
$$

provided that $n \rho \rightarrow \infty$ as $n \rightarrow \infty$.